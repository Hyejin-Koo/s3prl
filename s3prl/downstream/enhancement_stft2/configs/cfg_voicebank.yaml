runner:
  total_steps: 100000
  gradient_clipping: 1.0
  gradient_accumulate_steps: 1

  log_step: 1250
  eval_step: 1250
  save_step: 10000
  max_keep: 100
  eval_dataloaders:
    - dev

optimizer:
  name: AdamW
  lr: 1.e-3

downstream_expert:
  datarc:
    num_speakers: 1
    rate: 16000 #22050
    src: ['noisy']
    tgt: ['clean']
    n_fft: 512
    win_length: 400
    hop_length: 160
    window: "hann"
    center: True

  loaderrc:
    num_workers: 4
    train_batchsize: 12 #32
    eval_batchsize: 1  #Can't I make it multiple?
    train_dir: ./s3prl/downstream/enhancement_stft2/processed/AIhub/train
    dev_dir: ./s3prl/downstream/enhancement_stft2/processed/AIhub/dev
    test_dir: ./s3prl/downstream/enhancement_stft2/processed/AIhub/test
    inference_dir: /data/mixed_korean_recordroom_test
    description: 'recordroom Training set, enhance with wavlm, unfreeze wavLM !' #and UNI-directional LSTM'
  
  modelrc: 
    model: SepRNN
    rnn: LSTM
    rnn_layers: 3
    hidden_size: 256
    dropout: 0.1
    non_linear: sigmoid
    bidirectional: True
    loss_type: L1
    mask_type: AM
    log: log1p
